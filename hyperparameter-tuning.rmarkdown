---
title: "Lab 7: Hyperparameter Tuning"
author: "Genesis Lacy"
subtitle: "ESS 330 Quantitative Reasoning"
format: html
editor: visual
---



Reading in the data and cleaning it:



```{r}
library(tidyverse)
library(tidymodels)

root  <- 'https://gdex.ucar.edu/dataset/camels/file'

#Getting Basin Characteristics 
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

library(glue)
# Where the files live online
remote_files <- glue('{root}/camels_{types}.txt')

# where we want to download the data
local_files   <- glue('data/camels_{types}.txt')

library(purrr)
walk2(remote_files, local_files, download.file, quiet = TRUE)

library(readr)
library(MAP)
library(powerjoin)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <-power_full_join(camels ,by = 'gauge_id')

skimr::skim(camels)
```



Data splitting:



```{r}
library(rsample)
set.seed(42)

camels <- camels |> 
  mutate(logQmean = log(q_mean))

camels_split2 <- initial_split(camels, prop = 0.8)
camels_train2 <- training(camels_split2)
camels_test2  <- testing(camels_split2)


## filtering rows that have NA values
camels_train2 <- camels_train2 %>% filter(!is.na(q_mean))

camels_test2 <- camels_test2 %>% filter(!is.na(q_mean))

## filtering rows with Inf values 
camels_train2 <-camels_train2 %>% 
  filter(is.finite(q_mean))%>%
  filter(!is.na(q_mean))

camels_test2 <- camels_test2 %>%
  filter(is.finite(q_mean)) %>%
  filter(!is.na(q_mean))
  
```



Feature Engineering:



```{r}
## proper recipe
library(recipes)
camels_recipe <- recipe(q_mean ~ ., data = camels_train2) %>%
  step_rm(gauge_lat, gauge_lon) %>%
  step_novel(all_nominal_predictors()) %>%  # Handle unseen factor levels
  step_unknown(all_nominal_predictors()) %>%  # Handle missing categorical values
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())
 

# View the recipe blueprint
camels_recipe

# Prepping the recipe using training data
prepped_recipe <- prep(camels_recipe, training = camels_train2)

# Apply transformations while keeping gauge_lat and gauge_lon
train_ready <- bake(prepped_recipe, new_data = camels_train2)
test_ready <- bake(prepped_recipe, new_data = camels_test2)
```



Resampling and model testing:



```{r}

## building resamples with cross validation dataset (k-folds)
library(rsample)

new_camel_folds <- vfold_cv(camels_train2, v = 10)

## build 3 candidate regression models
library(tidymodels)

rf_spec <-rand_forest(trees = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")

boosted_spec <- boost_tree(learn_rate = tune(), trees = 500) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

lm_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
```



Testing the models:



```{r}
## test the models - workflow set, map and auto plot
library(workflows)

model_workflows <- workflow_set(
  preproc = list(recipe = camels_recipe),
  models = list(
    rf = rf_spec,
    boosted = boosted_spec,
    lm = lm_spec))

# mapping the models to recipe and re-samples
# fit models across k folds
library(tune)
new_fold_results <- workflow_map(
  model_workflows,
  resamples = new_camel_folds,
  grid = 10,
  metrics = metric_set(rmse, rsq),
  verbose = TRUE
)

autoplot(new_fold_results)

```



Model selection: Describe the reason for your choice with metrics. What is the model type, engine, and mode? Why do you think it's performing well for this problem?

I think that the model I will be using is boost_tree, the engine is xgboost and the mode is regression. The r squared value is extremely close to 1 and the root means squared error is also low which means that there are less errors.

Model tuning:



```{r}
## build model for chosen specification - define tunable model
library(tidymodels)

boosted_spec <- boost_tree(
  trees = 500,
  learn_rate = tune(),  # Tune learning rate
  tree_depth = tune()  # Tune tree depth
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

## create workflow
boosted_workflow <- workflow() %>%
  add_model(boosted_spec) %>%
  add_recipe(camels_recipe)

# Check workflow setup
boosted_workflow
## check the tunable values/ ranges
library(tune)

dials <- extract_parameter_set_dials(boosted_workflow)
# view available tunable parameters
dials$object
## define the search space - Latin hypercube SFD method, 25 predefined conditions

library(dials)

my.grid <- grid_latin_hypercube(dials, size = 25)

print(my.grid)
```



Tune the model:



```{r}
model_params <- tune_grid(
    boosted_workflow,
    resamples = new_camel_folds,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

autoplot(model_params)
```



Check the skill of the tuned model:



```{r}
## do this by collecting metrics/ show best/ describe in plain language
library(tune)

glimpse(camels_split2)

# Collect all metrics for different hyperparameter combinations
metrics_tuned <- collect_metrics(model_params)

# View the metrics tibble
print(metrics_tuned)

# selecting the best hyper parameter set
hp_best <- select_best(model_params, metric = "mae")
print(hp_best)

```



Finalize model and verify:



```{r}
## implement the last fit, interpret metrics, plot predictions
final_boosted_workflow <- finalize_workflow(boosted_workflow, hp_best)

# Check the finalized workflow
print(final_boosted_workflow)

 # fit final model
library(tune)

final_results <- last_fit(final_boosted_workflow, split = camels_split2, control = control_resamples(save_pred = TRUE))

#checking model performance 
collect_metrics(final_results)

#collecting predictions
predictions <- collect_predictions(final_results)

# View predictions tibble
print(predictions)
```



Note: since my predicted tibble has a zero value, I was not able to make graphs and I will accept any points off.

