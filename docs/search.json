[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "# loading in libraries\nlibrary(tidyverse)\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'scales' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n\n\n# loading in CAMELS data set \nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n# downloading documentation pdf\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\n#Getting Basin Characteristics \ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nlibrary(glue)\n# Where the files live online\nremote_files &lt;- glue('{root}/camels_{types}.txt')\n\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nlibrary(purrr)\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\nlibrary(readr)\nlibrary(MAP)\n\nWarning: package 'MAP' was built under R version 4.4.3\n\n\nLoading required package: flexmix\n\n\nWarning: package 'flexmix' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\n\nWarning: package 'lattice' was built under R version 4.4.3\n\n\nLoading required package: Matrix\n\n\nWarning: package 'Matrix' was built under R version 4.4.3\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(powerjoin)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;-power_full_join(camels ,by = 'gauge_id')\n\nQuestion 1: From the documentation PDF, report what zero_q_freq represents by making a map of the sites\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\nzero_q_freq represents the the frequency of days with no flow per day (mm).\nQuestion 2: Map Models\n\nlibrary(tidyverse)\n#model prep\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n# visual EDA\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n#testing log-log relationship\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# visualizing how a log trnasformation may benefit the q_mean data\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nModel Building\n\n# starting with splitting the data\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# pre processor, building recipe \n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n## fitting the lm to the data\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n## need to validate the lm_base on the test data; need to prep first, then bake and predict\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n#model eval: stat and vis\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\nUsing a workflow instead: using linear_reg function to define a linear regression model, adding the recipe to the workflow, fitting the training data and extracting the model coefficients\n\n# Define model\nlm_model &lt;- linear_reg() |&gt;\n  # define the engine\n  set_engine(\"lm\") |&gt;\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() |&gt;\n  # Add the recipe\n  add_recipe(rec) |&gt;\n  # Add the model\n  add_model(lm_model) |&gt;\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\nMaking predictions\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\nModel evaluation (default metrics): statistical and visual\n\n# creating a scatter plot of the observed vs predicted values\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nUsing a random forest model to predict mean stream flow:\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\nModel evaluation (logQmean): statistical and visual\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.588\n2 rsq     standard       0.740\n3 mae     standard       0.365\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nWorkflowset approach:\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.564  0.0253    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0260    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\nQuestion 3: Building XGBoost and Neural Network Model\n\n#defining the xgboost model\nxg_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\n\n# building neural network model\nlibrary(baguette)\nnn_model &lt;- bag_mlp() |&gt;\n  set_engine(\"nnet\") |&gt;\n  set_mode(\"regression\")\n\n# evaluate models and compare with linear and random forest models\nworkflow &lt;- workflow_set(list(rec),list(linear = lm_model,\n                                        xgb = xg_model,\n                                        rf = rf_model,\n                                        nnet = nn_model))|&gt;\n  \nworkflow_map('fit_resamples', resamples = camels_cv)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\nautoplot(workflow)\n\n\n\n\n\n\n\n\nWhich of the 4 models would you move forward with?\nI will be moving forward with the linear_reg (“lm_model”) model because it has the smallest range for rsq values.\nQuestion 4: Data prep/ Data splitting to predict mean Stream flow; experiment with other predictors\n\n# starting with splitting the data\n\nset.seed(123)\n\nstreamflow &lt;- q_mean &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\nstreamflow_split &lt;- initial_split(streamflow, prop = 0.75)\nstreamflow_train &lt;- training(streamflow_split)\nstreamflow_test  &lt;- testing(streamflow_split)\n\nstreamflow_fold &lt;- vfold_cv(streamflow_train, v = 10)\n\nModel Preparation:\n\ncamels |&gt; \n  select(runoff_ratio,baseflow_index, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n               runoff_ratio baseflow_index    q_mean\nrunoff_ratio      1.0000000      0.3452424 0.8755618\nbaseflow_index    0.3452424      1.0000000 0.2339068\nq_mean            0.8755618      0.2339068 1.0000000\n\n\nI will be trying to see if model I build will be able to predict mean flow using the runoff ratio and the base flow index. “runoff_ratio” tells us the ratio of mean daily discharge to mean daily precipitation. “baseflow_index” tells us about the daily stream discharge.\nRecipe:\n\n# Create a recipe to preprocess the data\nrec2 &lt;-  recipe(logQmean ~ runoff_ratio + baseflow_index, data = streamflow_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between runoff_ratio and baseflow_index\n  step_interact(terms = ~ runoff_ratio:baseflow_index) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n## fitting the lm to the data\n\n# Prepare the data\nbaked_stream_data &lt;- prep(rec2, streamflow_train) |&gt; \n  bake(new_data = NULL) |&gt;\n  drop_na()\n\nI will be choosing the runoff_ratio and baseflow_index as variables for the recipe.\nDefine 3 models (randomForest needs to be included):\n\nrf_model2 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nlm_model2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nnn_model2 &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nWorkflow set:\n\n# Interaction with lm\n# Base lm sets interaction terms with the * symbol\n\n## need to validate the lm_stream_base on the stream test data; need to prep first, then bake and predict\nbaked_stream_data &lt;- baked_stream_data %&gt;% drop_na()\n\nbaked_stream_data &lt;- baked_stream_data %&gt;%\n  mutate(across(where(is.numeric), ~ ifelse(is.na(.x), median(.x, na.rm = TRUE), .x)))\n\nlm_stream_base &lt;- lm(logQmean ~ runoff_ratio * baseflow_index, data = baked_stream_data)\nsummary(lm_stream_base)\n\n\nCall:\nlm(formula = logQmean ~ runoff_ratio * baseflow_index, data = baked_stream_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.71372 -0.12748  0.01362  0.18747  0.89405 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  1.50582    0.04917  30.622   &lt;2e-16 ***\nrunoff_ratio                 1.41391    0.02809  50.341   &lt;2e-16 ***\nbaseflow_index              -0.08859    0.06117  -1.448    0.148    \nrunoff_ratio:baseflow_index  0.01176    0.02380   0.494    0.621    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3093 on 498 degrees of freedom\nMultiple R-squared:  0.9331,    Adjusted R-squared:  0.9327 \nF-statistic:  2316 on 3 and 498 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ runoff_ratio + baseflow_index, data = baked_stream_data))\n\n\nCall:\nlm(formula = logQmean ~ runoff_ratio + baseflow_index, data = baked_stream_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.70926 -0.12782  0.01519  0.19012  0.87988 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1.48622    0.02905  51.166  &lt; 2e-16 ***\nrunoff_ratio    1.40346    0.01847  75.975  &lt; 2e-16 ***\nbaseflow_index -0.11415    0.03262  -3.499 0.000508 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.309 on 499 degrees of freedom\nMultiple R-squared:  0.9331,    Adjusted R-squared:  0.9328 \nF-statistic:  3479 on 2 and 499 DF,  p-value: &lt; 2.2e-16\n\nbaked_stream_data &lt;- prep(rec2, streamflow_train) |&gt; \n  bake(new_data = NULL)\n\nprepped_rec2 &lt;- prep(rec2, training = streamflow_train)\nbaked_stream_data &lt;- bake(prepped_rec2, new_data = streamflow_train)\n\n\nnew_wf &lt;- workflow_set(list(rec2), list(lm_model2, rf_model2, nn_model2)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nEvaluation and ranking results:\n\nautoplot(new_wf)\n\n\n\n\n\n\n\nrank_results(new_wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_linear_reg Prepro… rmse    0.309 0.0111     10 recipe       line…     1\n2 recipe_linear_reg Prepro… rsq     0.933 0.00514    10 recipe       line…     1\n3 recipe_bag_mlp    Prepro… rmse    0.310 0.0115     10 recipe       bag_…     2\n4 recipe_bag_mlp    Prepro… rsq     0.932 0.00429    10 recipe       bag_…     2\n5 recipe_rand_fore… Prepro… rmse    0.341 0.00913    10 recipe       rand…     3\n6 recipe_rand_fore… Prepro… rsq     0.918 0.00461    10 recipe       rand…     3\n\n\nI think that linear regression model is best because it has the closest r squared value to 1 .\nExtract and Evaluate:\n\neval_wf &lt;- workflow() %&gt;%\n  add_formula(logQmean ~ runoff_ratio + baseflow_index) %&gt;%\n  add_model(lm_model2)\n   \nfitted_eval_wf &lt;- eval_wf %&gt;%\n  fit(data = streamflow_train)\n\nvip::vip(extract_fit_parsnip(fitted_eval_wf))\n\n\n\n\n\n\n\npredictions &lt;- augment (fitted_eval_wf, new_data = streamflow_test) %&gt;%\n  mutate(diff = abs(logQmean - .pred))\n\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nWarning: package 'viridis' was built under R version 4.4.3\n\n\nLoading required package: viridisLite\n\n\n\nAttaching package: 'viridis'\n\n\nThe following object is masked from 'package:scales':\n\n    viridis_pal\n\nggplot(predictions, aes(x = .pred, y = logQmean)) +\n  geom_point(aes(color = abs(logQmean - .pred)), alpha = 0.7) +\n  scale_color_viridis_c(option = \"magma\") +\n  labs(\n    title = \"Observed vs. Predicted Streamflow\",\n    x = \"Predicted Log(Qmean)\",\n    y = \"Observed Log(Qmean)\",\n    color = \"Prediction Error\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Lab 7: Hyperparameter Tuning",
    "section": "",
    "text": "Reading in the data and cleaning it:\n\nlibrary(tidyverse)\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'scales' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n#Getting Basin Characteristics \ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\n# Where the files live online\nremote_files &lt;- glue('{root}/camels_{types}.txt')\n\n# where we want to download the data\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nlibrary(purrr)\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\nlibrary(readr)\nlibrary(MAP)\n\nWarning: package 'MAP' was built under R version 4.4.3\n\n\nLoading required package: flexmix\n\n\nWarning: package 'flexmix' was built under R version 4.4.3\n\n\nLoading required package: lattice\n\n\nWarning: package 'lattice' was built under R version 4.4.3\n\n\nLoading required package: Matrix\n\n\nWarning: package 'Matrix' was built under R version 4.4.3\n\n\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;-power_full_join(camels ,by = 'gauge_id')\n\nskimr::skim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1.00\n8\n8\n0\n671\n0\n\n\nhigh_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1.00\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n138\n0.79\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1.00\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\n\n\n\nData splitting:\n\nlibrary(rsample)\nset.seed(42)\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split2 &lt;- initial_split(camels, prop = 0.8)\ncamels_train2 &lt;- training(camels_split2)\ncamels_test2  &lt;- testing(camels_split2)\n\n\n## filtering rows that have NA values\ncamels_train2 &lt;- camels_train2 %&gt;% filter(!is.na(q_mean))\n\ncamels_test2 &lt;- camels_test2 %&gt;% filter(!is.na(q_mean))\n\n## filtering rows with Inf values \ncamels_train2 &lt;-camels_train2 %&gt;% \n  filter(is.finite(q_mean))%&gt;%\n  filter(!is.na(q_mean))\n\ncamels_test2 &lt;- camels_test2 %&gt;%\n  filter(is.finite(q_mean)) %&gt;%\n  filter(!is.na(q_mean))\n\nFeature Engineering:\n\n## proper recipe\nlibrary(recipes)\ncamels_recipe &lt;- recipe(q_mean ~ ., data = camels_train2) %&gt;%\n  step_rm(gauge_lat, gauge_lon) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%  # Handle unseen factor levels\n  step_unknown(all_nominal_predictors()) %&gt;%  # Handle missing categorical values\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n \n\n# View the recipe blueprint\ncamels_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 58\n\n\n\n\n\n── Operations \n\n\n• Variables removed: gauge_lat gauge_lon\n\n\n• Novel factor level assignment for: all_nominal_predictors()\n\n\n• Unknown factor level assignment for: all_nominal_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n# Prepping the recipe using training data\nprepped_recipe &lt;- prep(camels_recipe, training = camels_train2)\n\n# Apply transformations while keeping gauge_lat and gauge_lon\ntrain_ready &lt;- bake(prepped_recipe, new_data = camels_train2)\ntest_ready &lt;- bake(prepped_recipe, new_data = camels_test2)\n\nResampling and model testing:\n\n## building resamples with cross validation dataset (k-folds)\nlibrary(rsample)\n\nnew_camel_folds &lt;- vfold_cv(camels_train2, v = 10)\n\n## build 3 candidate regression models\nlibrary(tidymodels)\n\nrf_spec &lt;-rand_forest(trees = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nboosted_spec &lt;- boost_tree(learn_rate = tune(), trees = 500) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nTesting the models:\n\n## test the models - workflow set, map and auto plot\nlibrary(workflows)\n\nmodel_workflows &lt;- workflow_set(\n  preproc = list(recipe = camels_recipe),\n  models = list(\n    rf = rf_spec,\n    boosted = boosted_spec,\n    lm = lm_spec))\n\n# mapping the models to recipe and re-samples\n# fit models across k folds\nlibrary(tune)\nnew_fold_results &lt;- workflow_map(\n  model_workflows,\n  resamples = new_camel_folds,\n  grid = 10,\n  metrics = metric_set(rmse, rsq),\n  verbose = TRUE\n)\n\ni 1 of 3 tuning:     recipe_rf\n\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\n✔ 1 of 3 tuning:     recipe_rf (4m 48.6s)\n\n\ni 2 of 3 tuning:     recipe_boosted\n\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n✔ 2 of 3 tuning:     recipe_boosted (17m 46.1s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 3 of 3 resampling: recipe_lm\n\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\n\n\n\n\n\n✔ 3 of 3 resampling: recipe_lm (10.5s)\n\nautoplot(new_fold_results)\n\n\n\n\n\n\n\n\nModel selection: Describe the reason for your choice with metrics. What is the model type, engine, and mode? Why do you think it’s performing well for this problem?\nI think that the model I will be using is boost_tree, the engine is xgboost and the mode is regression. The r squared value is extremely close to 1 and the root means squared error is also low which means that there are less errors.\nModel tuning:\n\n## build model for chosen specification - define tunable model\nlibrary(tidymodels)\n\nboosted_spec &lt;- boost_tree(\n  trees = 500,\n  learn_rate = tune(),  # Tune learning rate\n  tree_depth = tune()  # Tune tree depth\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n## create workflow\nboosted_workflow &lt;- workflow() %&gt;%\n  add_model(boosted_spec) %&gt;%\n  add_recipe(camels_recipe)\n\n# Check workflow setup\nboosted_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_rm()\n• step_novel()\n• step_unknown()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 500\n  tree_depth = tune()\n  learn_rate = tune()\n\nComputational engine: xgboost \n\n## check the tunable values/ ranges\nlibrary(tune)\n\ndials &lt;- extract_parameter_set_dials(boosted_workflow)\n# view available tunable parameters\ndials$object\n\n[[1]]\nTree Depth (quantitative)\nRange: [1, 15]\n\n[[2]]\nLearning Rate (quantitative)\nTransformer: log-10 [1e-100, Inf]\nRange (transformed scale): [-3, -0.5]\n\n## define the search space - Latin hypercube SFD method, 25 predefined conditions\n\nlibrary(dials)\n\nmy.grid &lt;- grid_latin_hypercube(dials, size = 25)\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\nprint(my.grid)\n\n# A tibble: 25 × 2\n   tree_depth learn_rate\n        &lt;int&gt;      &lt;dbl&gt;\n 1          7    0.00277\n 2         13    0.133  \n 3         12    0.00166\n 4         12    0.0495 \n 5          6    0.00211\n 6          2    0.0142 \n 7          9    0.00123\n 8         10    0.0897 \n 9         14    0.267  \n10          7    0.00641\n# ℹ 15 more rows\n\n\nTune the model:\n\nmodel_params &lt;- tune_grid(\n    boosted_workflow,\n    resamples = new_camel_folds,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\nCheck the skill of the tuned model:\n\n## do this by collecting metrics/ show best/ describe in plain language\nlibrary(tune)\n\nglimpse(camels_split2)\n\nList of 4\n $ data  : tibble [671 × 59] (S3: tbl_df/tbl/data.frame)\n  ..$ gauge_id            : chr [1:671] \"01013500\" \"01022500\" \"01030500\" \"01031500\" ...\n  ..$ p_mean              : num [1:671] 3.13 3.61 3.27 3.52 3.32 ...\n  ..$ pet_mean            : num [1:671] 1.97 2.12 2.04 2.07 2.09 ...\n  ..$ p_seasonality       : num [1:671] 0.1879 -0.1145 0.0474 0.1041 0.1478 ...\n  ..$ frac_snow           : num [1:671] 0.313 0.245 0.277 0.292 0.28 ...\n  ..$ aridity             : num [1:671] 0.631 0.587 0.624 0.588 0.629 ...\n  ..$ high_prec_freq      : num [1:671] 12.9 20.6 17.1 18.9 20.1 ...\n  ..$ high_prec_dur       : num [1:671] 1.35 1.21 1.21 1.15 1.17 ...\n  ..$ high_prec_timing    : chr [1:671] \"son\" \"son\" \"son\" \"son\" ...\n  ..$ low_prec_freq       : num [1:671] 202 234 216 227 236 ...\n  ..$ low_prec_dur        : num [1:671] 3.43 3.66 3.51 3.47 3.69 ...\n  ..$ low_prec_timing     : chr [1:671] \"mam\" \"jja\" \"djf\" \"djf\" ...\n  ..$ geol_1st_class      : chr [1:671] \"Siliciclastic sedimentary rocks\" \"Acid plutonic rocks\" \"Siliciclastic sedimentary rocks\" \"Siliciclastic sedimentary rocks\" ...\n  ..$ glim_1st_class_frac : num [1:671] 0.816 0.591 0.573 0.449 0.308 ...\n  ..$ geol_2nd_class      : chr [1:671] \"Basic volcanic rocks\" \"Siliciclastic sedimentary rocks\" \"Metamorphics\" \"Metamorphics\" ...\n  ..$ glim_2nd_class_frac : num [1:671] 0.18 0.165 0.287 0.444 0.289 ...\n  ..$ carbonate_rocks_frac: num [1:671] 0 0 0.0521 0.0263 0 ...\n  ..$ geol_porostiy       : num [1:671] 0.1714 0.071 0.1178 0.0747 0.0522 ...\n  ..$ geol_permeability   : num [1:671] -14.7 -14.2 -14.5 -14.8 -14.5 ...\n  ..$ soil_depth_pelletier: num [1:671] 7.4 17.41 19.01 7.25 5.36 ...\n  ..$ soil_depth_statsgo  : num [1:671] 1.25 1.49 1.46 1.28 1.39 ...\n  ..$ soil_porosity       : num [1:671] 0.461 0.416 0.459 0.45 0.423 ...\n  ..$ soil_conductivity   : num [1:671] 1.11 2.38 1.29 1.37 2.62 ...\n  ..$ max_water_content   : num [1:671] 0.558 0.626 0.653 0.559 0.561 ...\n  ..$ sand_frac           : num [1:671] 27.8 59.4 32.2 35.3 55.2 ...\n  ..$ silt_frac           : num [1:671] 55.2 28.1 51.8 50.8 34.2 ...\n  ..$ clay_frac           : num [1:671] 16.3 12 14.8 12.7 10.3 ...\n  ..$ water_frac          : num [1:671] 5.377 1.227 1.634 0.675 0 ...\n  ..$ organic_frac        : num [1:671] 0.409 0 1.33 0 0 ...\n  ..$ other_frac          : num [1:671] 0 0.358 0.022 0 0.148 ...\n  ..$ gauge_lat           : num [1:671] 47.2 44.6 45.5 45.2 44.9 ...\n  ..$ gauge_lon           : num [1:671] -68.6 -67.9 -68.3 -69.3 -70 ...\n  ..$ elev_mean           : num [1:671] 250.3 92.7 143.8 247.8 310.4 ...\n  ..$ slope_mean          : num [1:671] 21.6 17.8 12.8 29.6 49.9 ...\n  ..$ area_gages2         : num [1:671] 2253 574 3676 769 909 ...\n  ..$ area_geospa_fabric  : num [1:671] 2304 620 3676 767 905 ...\n  ..$ frac_forest         : num [1:671] 0.906 0.923 0.878 0.955 0.991 ...\n  ..$ lai_max             : num [1:671] 4.17 4.87 4.69 4.9 5.09 ...\n  ..$ lai_diff            : num [1:671] 3.34 3.75 3.67 3.99 4.3 ...\n  ..$ gvf_max             : num [1:671] 0.805 0.864 0.859 0.871 0.891 ...\n  ..$ gvf_diff            : num [1:671] 0.372 0.338 0.351 0.399 0.445 ...\n  ..$ dom_land_cover_frac : num [1:671] 0.883 0.82 0.975 1 0.85 ...\n  ..$ dom_land_cover      : chr [1:671] \"    Mixed Forests\" \"    Mixed Forests\" \"    Mixed Forests\" \"    Mixed Forests\" ...\n  ..$ root_depth_50       : num [1:671] NA 0.237 NA 0.25 0.241 ...\n  ..$ root_depth_99       : num [1:671] NA 2.24 NA 2.4 2.34 ...\n  ..$ q_mean              : num [1:671] 1.7 2.17 1.82 2.03 2.18 ...\n  ..$ runoff_ratio        : num [1:671] 0.543 0.602 0.556 0.576 0.657 ...\n  ..$ slope_fdc           : num [1:671] 1.53 1.78 1.87 1.49 1.42 ...\n  ..$ baseflow_index      : num [1:671] 0.585 0.554 0.508 0.445 0.473 ...\n  ..$ stream_elas         : num [1:671] 1.85 1.7 1.38 1.65 1.51 ...\n  ..$ q5                  : num [1:671] 0.241 0.205 0.107 0.111 0.196 ...\n  ..$ q95                 : num [1:671] 6.37 7.12 6.85 8.01 8.1 ...\n  ..$ high_q_freq         : num [1:671] 6.1 3.9 12.2 18.9 14.9 ...\n  ..$ high_q_dur          : num [1:671] 8.71 2.29 7.21 3.29 2.58 ...\n  ..$ low_q_freq          : num [1:671] 41.4 65.2 89.2 94.8 71.5 ...\n  ..$ low_q_dur           : num [1:671] 20.2 17.1 19.4 14.7 12.8 ...\n  ..$ zero_q_freq         : num [1:671] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ hfd_mean            : num [1:671] 207 166 185 181 185 ...\n  ..$ logQmean            : num [1:671] 0.53 0.776 0.599 0.708 0.781 ...\n $ in_id : int [1:536] 561 321 153 74 228 146 634 49 128 303 ...\n $ out_id: logi NA\n $ id    : tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ id: chr \"Resample1\"\n - attr(*, \"class\")= chr [1:3] \"initial_split\" \"mc_split\" \"rsplit\"\n\n# Collect all metrics for different hyperparameter combinations\nmetrics_tuned &lt;- collect_metrics(model_params)\n\n# View the metrics tibble\nprint(metrics_tuned)\n\n# A tibble: 75 × 8\n   tree_depth learn_rate .metric .estimator   mean     n std_err .config        \n        &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n 1          7    0.00277 mae     standard   0.300     10 0.0194  Preprocessor1_…\n 2          7    0.00277 rmse    standard   0.507     10 0.0603  Preprocessor1_…\n 3          7    0.00277 rsq     standard   0.995     10 0.00222 Preprocessor1_…\n 4         13    0.133   mae     standard   0.0222    10 0.00627 Preprocessor1_…\n 5         13    0.133   rmse    standard   0.0721    10 0.0251  Preprocessor1_…\n 6         13    0.133   rsq     standard   0.997     10 0.00162 Preprocessor1_…\n 7         12    0.00166 mae     standard   0.507     10 0.0284  Preprocessor1_…\n 8         12    0.00166 rmse    standard   0.832     10 0.0777  Preprocessor1_…\n 9         12    0.00166 rsq     standard   0.993     10 0.00296 Preprocessor1_…\n10         12    0.0495  mae     standard   0.0175    10 0.00553 Preprocessor1_…\n# ℹ 65 more rows\n\n# selecting the best hyper parameter set\nhp_best &lt;- select_best(model_params, metric = \"mae\")\nprint(hp_best)\n\n# A tibble: 1 × 3\n  tree_depth learn_rate .config              \n       &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                \n1         12     0.0495 Preprocessor1_Model04\n\n\nFinalize model and verify:\n\n## implement the last fit, interpret metrics, plot predictions\nfinal_boosted_workflow &lt;- finalize_workflow(boosted_workflow, hp_best)\n\n# Check the finalized workflow\nprint(final_boosted_workflow)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_rm()\n• step_novel()\n• step_unknown()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 500\n  tree_depth = 12\n  learn_rate = 0.0494820467834889\n\nComputational engine: xgboost \n\n # fit final model\nlibrary(tune)\n\nfinal_results &lt;- last_fit(final_boosted_workflow, split = camels_split2, control = control_resamples(save_pred = TRUE))\n\n→ A | error:   [13:10:42] src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more\ninformation.\n\n\nWarning: Unknown or uninitialised column: `.extracts`.\n\n#checking model performance \ncollect_metrics(final_results)\n\nNULL\n\n#collecting predictions\npredictions &lt;- collect_predictions(final_results)\n\n# View predictions tibble\nprint(predictions)\n\n# A tibble: 0 × 1\n# ℹ 1 variable: id &lt;chr&gt;\n\n\nNote: since my predicted tibble has a zero value, I was not able to make graphs and I will accept any points off."
  }
]