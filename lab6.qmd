---
title: "Lab 6: Machine Learning in Hydrology"
author: "Genesis Lacy"
subtitle: "ESS 330 Quantitative Reasoning"
format: html
editor: visual
---

```{r}
# loading in libraries
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
```

```{r}
# loading in CAMELS data set 
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

# downloading documentation pdf
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

```

```{r}
#Getting Basin Characteristics 
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

library(glue)
# Where the files live online
remote_files <- glue('{root}/camels_{types}.txt')

# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

library(purrr)
walk2(remote_files, local_files, download.file, quiet = TRUE)

library(readr)
library(MAP)
library(powerjoin)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <-power_full_join(camels ,by = 'gauge_id')


```

**Question 1: From the documentation PDF, report what zero_q_freq represents by making a map of the sites**

```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```

zero_q_freq represents the the frequency of days with no flow per day (mm).

**Question 2: Map Models**

```{r}
library(tidyverse)
#model prep
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()

# visual EDA

# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  # Add points colored by mean flow
  geom_point(aes(color = q_mean)) +
  # Add a linear regression line
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  # Apply the viridis color scale
  scale_color_viridis_c() +
  # Add a title, axis labels, and theme (w/ legend on the bottom)
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
#testing log-log relationship

ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
# visualizing how a log trnasformation may benefit the q_mean data

ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 

```

Model Building

```{r}
# starting with splitting the data

set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)

# pre processor, building recipe 

# Create a recipe to preprocess the data
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

## fitting the lm to the data

# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

# Interaction with lm
#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)


# Sanity Interaction term from recipe ... these should be equal!!
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))


## need to validate the lm_base on the test data; need to prep first, then bake and predict

test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)


#model eval: stat and vis

metrics(test_data, truth = logQmean, estimate = lm_pred)

ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  # Apply a gradient color scale
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

Using a workflow instead: using linear_reg function to define a linear regression model, adding the recipe to the workflow, fitting the training data and extracting the model coefficients

```{r}
# Define model
lm_model <- linear_reg() |>
  # define the engine
  set_engine("lm") |>
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() |>
  # Add the recipe
  add_recipe(rec) |>
  # Add the model
  add_model(lm_model) |>
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients

# From the base implementation
summary(lm_base)$coefficients
```

Making predictions

```{r}
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```

Model evaluation (default metrics): statistical and visual

```{r}
# creating a scatter plot of the observed vs predicted values
metrics(lm_data, truth = logQmean, estimate = .pred)

ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

Using a random forest model to predict mean stream flow:

```{r}
library(baguette)
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model
  fit(data = camels_train) 


rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```

Model evaluation (logQmean): statistical and visual

```{r}
metrics(rf_data, truth = logQmean, estimate = .pred)

ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

Workflowset approach:

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)

rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

**Question 3: Building XGBoost and Neural Network Model**

```{r}
#defining the xgboost model
xg_model <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("regression")


# building neural network model
library(baguette)
nn_model <- bag_mlp() |>
  set_engine("nnet") |>
  set_mode("regression")

# evaluate models and compare with linear and random forest models
workflow <- workflow_set(list(rec),list(linear = lm_model,
                                        xgb = xg_model,
                                        rf = rf_model,
                                        nnet = nn_model))|>
  
workflow_map('fit_resamples', resamples = camels_cv)
autoplot(workflow)
```

Which of the 4 models would you move forward with?

I will be moving forward with the linear_reg ("lm_model") model because it has the smallest range for rsq values.

**Question 4: Data prep/ Data splitting to predict mean Stream flow; experiment with other predictors**

```{r}
# starting with splitting the data

set.seed(123)

streamflow <- q_mean <- camels |> 
  mutate(logQmean = log(q_mean))

streamflow_split <- initial_split(streamflow, prop = 0.75)
streamflow_train <- training(streamflow_split)
streamflow_test  <- testing(streamflow_split)

streamflow_fold <- vfold_cv(streamflow_train, v = 10)
```

Model Preparation:

```{r}
camels |> 
  select(runoff_ratio,slope_fdc, q_mean) |> 
  drop_na() |> 
  cor()
```

I will be trying to see if model I build will be able to predict mean flow using the runoff ratio and the slope. "runoff_ratio" tells us the ratio of mean daily discharge to mean daily precipitation. "slope_fdc" tells us the slope of the flow duration curve.

Recipe:

```{r}
# Create a recipe to preprocess the data
rec2 <-  recipe(logQmean ~ runoff_ratio + slope_fdc, data = streamflow_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ runoff_ratio:slope_fdc) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

## fitting the lm to the data

# Prepare the data
baked_stream_data <- prep(rec2, streamflow_train) |> 
  bake(new_data = NULL)
```

I will be choosing the soil porosity and water_frac (the fraction of the top 1.5m marked as water) variables for the recipe.

Define 3 models (randomForest needs to be included):

```{r}

rf_model2 <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

lm_model2 <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

nn_model2 <- bag_mlp() |>
  set_engine("nnet") |>
  set_mode("regression")
```

Workflow set:

```{r}
# Interaction with lm
#  Base lm sets interaction terms with the * symbol

# Changing Inf and -Inf values
baked_stream_data <- baked_stream_data %>%
  mutate(across(everything(), ~ replace(.x, is.infinite(.x), max(.x[!is.infinite(.x)], na.rm = TRUE))))


lm_stream_base <- lm(logQmean ~ runoff_ratio * slope_fdc, data = baked_stream_data)
summary(lm_stream_base)


# Sanity Interaction term from recipe ... these should be equal!!
summary(lm(logQmean ~ runoff_ratio + slope_fdc, data = baked_stream_data))


## need to validate the lm_stream_base on the stream test data; need to prep first, then bake and predict

stream_test_data <- bake(prep(rec2), new_stream_data = streamflow_test)
stream_test_data$lm_pred <- predict(lm_base, streamflownewdata = stream_test_data)


new_wf <- workflow_set(list(rec2), list(lm_model2, rf_model2, nn_model2)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)
```

Evaluation:

```{r}
autoplot(new_wf)

rank_results(new_wf, rank_metric = "rsq", select_best = TRUE)
```

I think that \_\_\_\_ model is best because of \_\_\_\_\_\_.

Extract and Evaluate:

```{r}
fit <- new_wf() |>
  add_recipe(rec2) |>
  add_model(lm_model2) |>
  fit(data = streamflow_train)

vip::vip(fit)

predictions <- augment (fit, new_data = streamflow_test) |>
  mutate(diff = abs(logC, estimate = .pred))
```
