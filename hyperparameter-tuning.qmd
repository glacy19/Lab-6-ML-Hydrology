---
title: "Lab 7: Hyperparameter Tuning"
author: "Genesis Lacy"
subtitle: "ESS 330 Quantitative Reasoning"
format: html
editor: visual
---

Reading in the data and cleaning it:

```{r}
library(tidyverse)
library(tidymodels)

root  <- 'https://gdex.ucar.edu/dataset/camels/file'

#Getting Basin Characteristics 
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

library(glue)
# Where the files live online
remote_files <- glue('{root}/camels_{types}.txt')

# where we want to download the data
local_files   <- glue('data/camels_{types}.txt')

library(purrr)
walk2(remote_files, local_files, download.file, quiet = TRUE)

library(readr)
library(MAP)
library(powerjoin)
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <-power_full_join(camels ,by = 'gauge_id')

skimr::skim(camels)

```

Data splitting:

```{r}
library(rsample)
set.seed(42)

camels <- camels |> 
  mutate(logQmean = log(q_mean))

camels_split2 <- initial_split(camels, prop = 0.8)
camels_train2 <- training(camels_split2)
camels_test2  <- testing(camels_split2)
```

Feature Engineering:

```{r}
## proper recipe
library(recipes)

# Create recipe for predicting q_mean
camels_recipe <- recipe(q_mean ~ ., data = camels_train2) %>%
  step_rm(gauge_lat, gauge_lon) %>%  # Remove spatial columns from predictors
  step_normalize(all_numeric_predictors()) %>%  # Standardize numeric features
  step_dummy(all_nominal_predictors()) %>%  # Convert categorical variables into dummies
  step_zv(all_predictors())  # Remove zero-variance predictors

# View the recipe blueprint
camels_recipe

# Prepping the recipe using training data
prepped_recipe <- prep(camels_recipe, training = camels_train2)

# Apply transformations while keeping gauge_lat and gauge_lon
train_ready <- bake(prepped_recipe, new_data = camels_train2)
test_ready <- bake(prepped_recipe, new_data = camels_test2)

```

Resampling and model testing:

```{r}

## building resamples with cross validation dataset (k-folds)
library(rsample)

new_camel_folds <- vfold_cv(camels_train2, v = 10)

## build 3 candidate regression models
library(tidymodels)

rf_spec <-rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")

boosted_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

mlp_spec <- mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")
```

Testing the models:

```{r}
## test the models - workflow set, map and autoplot
library(workflows)

model_workflows <- workflow_set(
  preproc = list(recipe = camels_recipe),
  models = list(
    rf = rf_spec,
    boosted = boosted_spec,
    mlp = mlp_spec
  )
)

# mapping the models to recipe and resamples
# fit models acrros k folds
library(tune)
new_fold_results <- workflow_map(
  model_workflows,
  resamples = new_camel_folds,
  grid = 10,
  metrics = metric_set(rmse, rsq),
  verbose = TRUE
)

autoplot(new_fold_results)

## model selection with justification
```

Model tuning:

```{r}
## build model for chosen specification


## create workflow

## check the tunable values/ ranges

## define the search space


```

Tune the model:

```{r}
## tune the model - code used from class

model_params <-  tune_grid(
    wf_tune,
    resamples = folds,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

autoplot(model_params)
```

Check the skill of the tuned model:

```{r}
## do this by collecting metrics/ show best/ describe in plain language
```

Finalize model and verify:

```{r}
## implenent the last fit, interpret metrics, plot predictions

```

Building a map (use conus):

```{r}

```
